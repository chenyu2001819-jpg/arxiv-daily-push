#!/usr/bin/env python3
"""
arXiv æ¯æ—¥æ–‡ç« æ¨é€æ™ºèƒ½ä½“
æ ¹æ®å…³é”®è¯è‡ªåŠ¨æŠ“å– arXiv è®ºæ–‡ï¼Œæ¯å¤©æ¨é€æœ€å¤š30ç¯‡ç›¸å…³æ–‡ç« 
æ”¯æŒé‚®ä»¶æ¨é€åŠŸèƒ½ï¼Œæ”¯æŒ GitHub Actions éƒ¨ç½²
æ”¯æŒå¼•ç”¨æ¬¡æ•°æ’åº
"""

import os
import re
import yaml
import json
import logging
import feedparser
import requests
import argparse
from datetime import datetime, timedelta
from typing import List, Dict, Set, Tuple, Optional
from dataclasses import dataclass, field
from pathlib import Path

# å¯¼å…¥é‚®ä»¶å‘é€æ¨¡å—
try:
    from email_sender import EmailSender
    EMAIL_AVAILABLE = True
except ImportError:
    EMAIL_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class Paper:
    """è®ºæ–‡æ•°æ®ç»“æ„"""
    title: str
    authors: List[str]
    summary: str
    link: str
    pdf_link: str
    published: datetime
    categories: List[str]
    primary_category: str
    arxiv_id: str = ""  # arXiv ID
    score: float = 0.0  # ç›¸å…³æ€§å¾—åˆ†
    citation_count: int = 0  # å¼•ç”¨æ¬¡æ•°
    matched_keywords: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        return {
            'title': self.title,
            'authors': self.authors,
            'summary': self.summary[:500] + '...' if len(self.summary) > 500 else self.summary,
            'link': self.link,
            'pdf_link': self.pdf_link,
            'published': self.published.strftime('%Y-%m-%d'),
            'categories': self.categories,
            'primary_category': self.primary_category,
            'arxiv_id': self.arxiv_id,
            'score': round(self.score, 2),
            'citation_count': self.citation_count,
            'matched_keywords': self.matched_keywords
        }


class KeywordManager:
    """å…³é”®è¯ç®¡ç†å™¨"""
    
    def __init__(self, keywords_file: str = "keywords.txt"):
        self.keywords_file = keywords_file
        self.keywords = []
        self.keyword_groups = {}
        self.core_keywords = []  # æ ¸å¿ƒå…³é”®è¯
        self.extended_keywords = []  # æ‰©å±•å…³é”®è¯
        self.search_queries = []  # æœç´¢æŸ¥è¯¢è¯
        self._load_keywords()
    
    def _load_keywords(self):
        """ä»æ–‡ä»¶åŠ è½½å…³é”®è¯"""
        if not os.path.exists(self.keywords_file):
            raise FileNotFoundError(f"å…³é”®è¯æ–‡ä»¶ä¸å­˜åœ¨: {self.keywords_file}")
        
        with open(self.keywords_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        current_group = "default"
        is_extended_section = False
        self.keyword_groups[current_group] = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æµ‹åˆ†ç»„æ ‡é¢˜
            if 'å…³é”®è¯' in line or 'æ‰©å±•' in line.lower():
                current_group = line
                self.keyword_groups[current_group] = []
                if 'æ‰©å±•' in line.lower():
                    is_extended_section = True
                continue
            
            # å¤„ç†ä¸€è¡Œå¤šä¸ªå…³é”®è¯çš„æƒ…å†µ
            sub_keywords = re.split(r'[\sã€,ï¼Œ]+', line)
            for kw in sub_keywords:
                kw = kw.strip()
                if kw and len(kw) > 1:
                    self.keywords.append(kw)
                    self.keyword_groups[current_group].append(kw)
                    
                    # åŒºåˆ†æ ¸å¿ƒå…³é”®è¯å’Œæ‰©å±•å…³é”®è¯
                    if is_extended_section:
                        self.extended_keywords.append(kw)
                    else:
                        self.core_keywords.append(kw)
        
        # å»é‡
        self.keywords = list(set(self.keywords))
        self.core_keywords = list(set(self.core_keywords))
        self.extended_keywords = list(set(self.extended_keywords))
        
        # ç”Ÿæˆæœç´¢æŸ¥è¯¢
        self._generate_search_queries()
        
        logger.info(f"åŠ è½½äº† {len(self.keywords)} ä¸ªå…³é”®è¯")
        logger.info(f"  - æ ¸å¿ƒå…³é”®è¯: {len(self.core_keywords)} ä¸ª")
        logger.info(f"  - æ‰©å±•å…³é”®è¯: {len(self.extended_keywords)} ä¸ª")
        logger.info(f"  - æœç´¢æŸ¥è¯¢: {len(self.search_queries)} ä¸ª")
    
    def _generate_search_queries(self):
        """ç”Ÿæˆ arXiv æœç´¢æŸ¥è¯¢è¯"""
        # å…³é”®è¯ç¿»è¯‘æ˜ å°„
        translations = {
            # äº§ä¸šç»„ç»‡
            'ç©ºè°ƒå¸‚åœº': 'air conditioner market',
            'air conditioner market': 'air conditioner market',
            'ç”µåŠ¨æ±½è½¦å¸‚åœº': 'electric vehicle market',
            'ç”µè½¦å¸‚åœº': 'EV market',
            'è€ç”¨æ¶ˆè´¹å“': 'durable goods',
            'å®è¯äº§ä¸šç»„ç»‡': 'empirical industrial organization',
            'å®è¯ io': 'empirical IO',
            'å¸‚åœºç»“æ„': 'market structure',
            'äº§å“å·®å¼‚åŒ–': 'product differentiation',
            'éœ€æ±‚ä¼°è®¡': 'demand estimation',
            'ä¾›ç»™è¡Œä¸º': 'supply behavior',
            'å®šä»·ç­–ç•¥': 'pricing strategy',
            'å¸‚åœºåŠ¿åŠ›': 'market power',
            'ç¦åˆ©åˆ†æ': 'welfare analysis',
            'å®¶ç”µå¸‚åœº': 'appliance market',
            'æ–°èƒ½æºæ±½è½¦å¸‚åœº': 'new energy vehicle market',
            'ç¦»æ•£é€‰æ‹©æ¨¡å‹': 'discrete choice model',
            'blp æ¨¡å‹': 'BLP model',
            'ç»“æ„ä¼°è®¡': 'structural estimation',
            'å¯¡å¤´ç«äº‰': 'oligopoly competition',
            'çºµå‘å…³ç³»': 'vertical relationship',
            'æŠ€æœ¯åˆ›æ–°': 'technological innovation',
            'æ”¿ç­–è¯„ä¼°': 'policy evaluation',
            'æ¶ˆè´¹è¡Œä¸º': 'consumer behavior',
            # èˆªè¿ç›¸å…³
            'åŒ—æèˆªé“': 'Arctic shipping',
            'åŒ—æèˆªè¿': 'Arctic shipping',
            'å…¨çƒèˆªè¿è´¸æ˜“': 'global shipping trade',
            'æµ·è¿ç¢³æ’æ”¾': 'maritime carbon emission',
            'èˆªè¿å‡æ’': 'shipping emission reduction',
            'èˆ¹èˆ¶ç¢³æ’æ”¾': 'vessel carbon emission',
            'ç¢³å‡æ’æ”¿ç­–': 'carbon reduction policy',
            'èˆªè¿ç¢³è¶³è¿¹': 'shipping carbon footprint',
            'ç»¿è‰²èˆªè¿': 'green shipping',
            'æ°”å€™å½±å“': 'climate impact',
            'å›½é™…æµ·è¿': 'international shipping',
            'æµ·è¿è´¸æ˜“æ ¼å±€': 'maritime trade pattern',
            'ç¢³ç¨': 'carbon tax',
            'ç¢³å¸‚åœº': 'carbon market',
            'èˆ¹èˆ¶èƒ½æ•ˆ': 'ship energy efficiency',
            'ä½ç¢³èˆªè¿': 'low carbon shipping',
            'åŒ—æç¯å¢ƒå½±å“': 'Arctic environmental impact',
            'è´¸æ˜“è·¯çº¿ä¼˜åŒ–': 'trade route optimization',
            'å¯æŒç»­èˆªè¿': 'sustainable shipping',
        }
        
        queries = set()
        all_keywords = self.core_keywords + self.extended_keywords
        
        for kw in all_keywords:
            kw_lower = kw.lower()
            # ç›´æ¥ä½¿ç”¨è‹±æ–‡å…³é”®è¯
            if kw_lower.isascii():
                queries.add(kw_lower)
            # ä½¿ç”¨ç¿»è¯‘åçš„è‹±æ–‡
            elif kw in translations:
                queries.add(translations[kw])
        
        self.search_queries = list(queries)
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æœç´¢è¯ï¼Œä½¿ç”¨é»˜è®¤æœç´¢
        if not self.search_queries:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è‹±æ–‡æœç´¢è¯ï¼Œä½¿ç”¨é»˜è®¤æœç´¢")
            self.search_queries = [
                'industrial organization',
                'market structure',
                'shipping',
                'carbon emission'
            ]
    
    def get_search_queries(self) -> List[str]:
        """è·å–æœç´¢æŸ¥è¯¢è¯"""
        return self.search_queries
    
    def calculate_match_score(self, title: str, summary: str) -> Tuple[float, List[str]]:
        """
        è®¡ç®—æ–‡ç« ä¸å…³é”®è¯çš„åŒ¹é…å¾—åˆ†
        
        è¿”å›: (å¾—åˆ†, åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨)
        """
        text = (title + " " + summary).lower()
        title_lower = title.lower()
        score = 0.0
        matched = []
        
        # æ ¸å¿ƒå…³é”®è¯åŒ¹é…ï¼ˆæƒé‡æ›´é«˜ï¼‰
        for kw in self.core_keywords:
            kw_lower = kw.lower()
            # æ£€æŸ¥è‹±æ–‡å½¢å¼
            if kw_lower in text:
                matched.append(kw)
                if kw_lower in title_lower:
                    score += 5.0
                else:
                    score += 2.0
            # æ£€æŸ¥æ˜¯å¦ä¸ºè‹±æ–‡å•è¯ï¼ˆæ›´å®½æ¾çš„åŒ¹é…ï¼‰
            elif kw_lower.replace(' ', '') in text.replace(' ', ''):
                matched.append(kw)
                score += 1.0
        
        # æ‰©å±•å…³é”®è¯åŒ¹é…
        for kw in self.extended_keywords:
            kw_lower = kw.lower()
            if kw_lower in text and kw not in matched:
                matched.append(kw)
                if kw_lower in title_lower:
                    score += 2.0
                else:
                    score += 0.5
        
        return score, matched


class CitationFetcher:
    """å¼•ç”¨æ¬¡æ•°è·å–å™¨ - ä½¿ç”¨ Semantic Scholar API"""
    
    API_URL = "https://api.semanticscholar.org/graph/v1/paper/"
    
    def __init__(self, timeout: int = 10):
        self.timeout = timeout
    
    def get_citation_count(self, arxiv_id: str) -> int:
        """è·å–è®ºæ–‡çš„å¼•ç”¨æ¬¡æ•°"""
        if not arxiv_id:
            return 0
        
        try:
            url = f"{self.API_URL}arXiv:{arxiv_id}"
            params = {'fields': 'citationCount'}
            
            response = requests.get(url, params=params, timeout=self.timeout)
            
            if response.status_code == 200:
                data = response.json()
                count = data.get('citationCount', 0)
                return count if count else 0
            else:
                return 0
                
        except Exception as e:
            return 0
    
    def batch_get_citations(self, papers: List[Paper]) -> None:
        """æ‰¹é‡è·å–å¼•ç”¨æ¬¡æ•°"""
        logger.info(f"æ­£åœ¨è·å– {len(papers)} ç¯‡è®ºæ–‡çš„å¼•ç”¨æ¬¡æ•°...")
        
        for i, paper in enumerate(papers):
            if paper.arxiv_id:
                paper.citation_count = self.get_citation_count(paper.arxiv_id)
                if (i + 1) % 10 == 0:
                    logger.info(f"  å·²å¤„ç† {i + 1}/{len(papers)} ç¯‡")
                import time
                time.sleep(0.5)
        
        logger.info("å¼•ç”¨æ¬¡æ•°è·å–å®Œæˆ")


class ArxivSearcher:
    """arXiv æœç´¢å™¨"""
    
    ARXIV_API_URL = "http://export.arxiv.org/api/query"
    
    def __init__(self, max_results_per_query: int = 50):
        self.max_results_per_query = max_results_per_query
    
    def search(self, query: str, days_back: int = 7) -> List[Paper]:
        """æœç´¢ arXiv æ–‡ç« """
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        params = {
            'search_query': f'all:{query}',
            'start': 0,
            'max_results': self.max_results_per_query,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        try:
            logger.info(f"æœç´¢ arXiv: {query}")
            response = requests.get(
                self.ARXIV_API_URL, 
                params=params, 
                timeout=30
            )
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            papers = []
            
            for entry in feed.entries:
                published = datetime.strptime(
                    entry.published, 
                    '%Y-%m-%dT%H:%M:%SZ'
                )
                
                if published < start_date:
                    continue
                
                pdf_link = ""
                for link in entry.links:
                    if link.get('type') == 'application/pdf':
                        pdf_link = link.href
                        break
                
                authors = [author.name for author in entry.get('authors', [])]
                categories = [tag.term for tag in entry.get('tags', [])]
                primary_cat = entry.get('arxiv_primary_category', {}).get('term', '')
                
                # æå– arXiv ID
                arxiv_id = ""
                if '/abs/' in entry.link:
                    arxiv_id = entry.link.split('/abs/')[-1].split('v')[0]
                
                paper = Paper(
                    title=entry.title.replace('\n', ' ').strip(),
                    authors=authors,
                    summary=entry.summary.replace('\n', ' ').strip(),
                    link=entry.link,
                    pdf_link=pdf_link,
                    published=published,
                    categories=categories,
                    primary_category=primary_cat,
                    arxiv_id=arxiv_id
                )
                papers.append(paper)
            
            logger.info(f"  æ‰¾åˆ° {len(papers)} ç¯‡æ–‡ç« ")
            return papers
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥ '{query}': {e}")
            return []


class PaperRanker:
    """æ–‡ç« æ’åºå™¨"""
    
    CAT_PREFIXES = ['econ.', 'q-fin.', 'stat.', 'cs.']
    
    def __init__(self, keyword_manager: KeywordManager):
        self.keyword_manager = keyword_manager
    
    def rank_papers(self, papers: List[Paper], sort_by_citations: bool = False) -> List[Paper]:
        """
        å¯¹æ–‡ç« è¿›è¡Œæ’åº
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            sort_by_citations: æ˜¯å¦æŒ‰å¼•ç”¨æ¬¡æ•°æ’åº
        """
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        for paper in papers:
            score, matched = self.keyword_manager.calculate_match_score(
                paper.title, paper.summary
            )
            paper.score = score
            paper.matched_keywords = matched
            
            # åˆ†ç±»ç›¸å…³æ€§åŠ åˆ†
            for cat in paper.categories:
                for prefix in self.CAT_PREFIXES:
                    if cat.startswith(prefix):
                        paper.score += 0.5
                        break
            
            # æ—¶æ•ˆæ€§åŠ åˆ†
            days_since = (datetime.now() - paper.published).days
            if days_since <= 1:
                paper.score += 2.0
            elif days_since <= 3:
                paper.score += 1.0
        
        # æ’åº
        if sort_by_citations:
            papers.sort(key=lambda p: (-p.citation_count, -p.score))
        else:
            papers.sort(key=lambda p: -p.score)
        
        return papers


def load_config_from_env() -> Dict:
    """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
    config = {}
    
    email_enabled = os.environ.get('EMAIL_ENABLED', '').lower()
    if email_enabled in ('true', '1', 'yes'):
        config['email'] = {
            'enabled': True,
            'sender_email': os.environ.get('EMAIL_SENDER', ''),
            'sender_password': os.environ.get('EMAIL_PASSWORD', ''),
            'receiver_emails': os.environ.get('EMAIL_RECEIVERS', '').split(','),
            'smtp_host': os.environ.get('SMTP_HOST', ''),
            'smtp_port': int(os.environ.get('SMTP_PORT', '465') or '465'),
            'use_ssl': os.environ.get('USE_SSL', 'true').lower() == 'true',
            'use_tls': os.environ.get('USE_TLS', 'false').lower() == 'true',
        }
        config['email']['receiver_emails'] = [
            email.strip() for email in config['email']['receiver_emails'] 
            if email.strip()
        ]
    
    if os.environ.get('MAX_PAPERS'):
        config['max_papers_per_day'] = int(os.environ['MAX_PAPERS'])
    if os.environ.get('DAYS_BACK'):
        config['days_back'] = int(os.environ['DAYS_BACK'])
    if os.environ.get('MIN_SCORE'):
        config['min_score_threshold'] = float(os.environ['MIN_SCORE'])
    if os.environ.get('SORT_BY_CITATIONS'):
        config['sort_by_citations'] = os.environ['SORT_BY_CITATIONS'].lower() == 'true'
    
    return config


class ArxivAgent:
    """arXiv æ–‡ç« æ¨é€æ™ºèƒ½ä½“ä¸»ç±»"""
    
    def __init__(self, config_file: str = "config.yaml"):
        self.config = self._load_config(config_file)
        self.keyword_manager = KeywordManager(self.config.get('keywords_file', 'keywords.txt'))
        self.searcher = ArxivSearcher(
            max_results_per_query=self.config.get('max_results_per_query', 50)
        )
        self.ranker = PaperRanker(self.keyword_manager)
        self.citation_fetcher = CitationFetcher()
        
        # é‚®ä»¶å‘é€å™¨
        self.email_sender: Optional[EmailSender] = None
        if EMAIL_AVAILABLE and self.config.get('email', {}).get('enabled', False):
            try:
                self.email_sender = EmailSender(self.config['email'])
                logger.info("âœ… é‚®ä»¶å‘é€åŠŸèƒ½å·²å¯ç”¨")
            except Exception as e:
                logger.error(f"é‚®ä»¶å‘é€å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # å»é‡å­˜å‚¨
        self.seen_ids: Set[str] = set()
        self.history_file = self.config.get('history_file', 'paper_history.json')
        self._load_history()
    
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'keywords_file': 'keywords.txt',
            'max_results_per_query': 50,
            'max_papers_per_day': 30,
            'days_back': 7,
            'output_dir': 'daily_papers',
            'history_file': 'paper_history.json',
            'min_score_threshold': 1.0,  # é»˜è®¤é˜ˆå€¼è¾ƒä½ï¼Œç¡®ä¿æœ‰æ–‡ç« 
            'sort_by_citations': False,
            'fetch_citations': False,
            'email': {'enabled': False}
        }
        
        # åŠ è½½ YAML é…ç½®
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                yaml_config = yaml.safe_load(f)
                if yaml_config:
                    default_config.update(yaml_config)
        
        # åŠ è½½ç¯å¢ƒå˜é‡é…ç½®
        env_config = load_config_from_env()
        if env_config:
            logger.info("ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®")
            default_config.update(env_config)
        
        return default_config
    
    def _load_history(self):
        """åŠ è½½å·²æ¨é€æ–‡ç« å†å²"""
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
                    self.seen_ids = set(history.get('paper_ids', []))
                    logger.info(f"åŠ è½½å†å²è®°å½•: {len(self.seen_ids)} ç¯‡æ–‡ç« ")
            except Exception as e:
                logger.warning(f"åŠ è½½å†å²è®°å½•å¤±è´¥: {e}")
    
    def _save_history(self):
        """ä¿å­˜å·²æ¨é€æ–‡ç« å†å²"""
        history = {
            'paper_ids': list(self.seen_ids),
            'last_update': datetime.now().isoformat()
        }
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    
    def _get_paper_id(self, paper: Paper) -> str:
        """ç”Ÿæˆæ–‡ç« å”¯ä¸€ID"""
        if paper.arxiv_id:
            return paper.arxiv_id
        return paper.title[:50]
    
    def run(self, send_email: bool = True) -> str:
        """æ‰§è¡Œæ¯æ—¥æ–‡ç« æŠ“å–å’Œæ¨é€"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ‰§è¡Œ arXiv æ–‡ç« æ¨é€ä»»åŠ¡")
        logger.info("=" * 60)
        
        # 1. è·å–æ‰€æœ‰æœç´¢è¯
        queries = self.keyword_manager.get_search_queries()
        logger.info(f"æœç´¢æŸ¥è¯¢: {queries}")
        
        # 2. æœç´¢æ–‡ç« 
        all_papers: Dict[str, Paper] = {}
        days_back = self.config.get('days_back', 7)
        
        for query in queries:
            papers = self.searcher.search(query, days_back=days_back)
            for paper in papers:
                paper_id = self._get_paper_id(paper)
                if paper_id not in all_papers:
                    all_papers[paper_id] = paper
            
            import time
            time.sleep(1)
        
        logger.info(f"å…±æ‰¾åˆ° {len(all_papers)} ç¯‡ä¸é‡å¤æ–‡ç« ")
        
        if not all_papers:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œè¯·æ£€æŸ¥å…³é”®è¯é…ç½®")
            return ""
        
        # 3. è¿‡æ»¤å·²æ¨é€çš„æ–‡ç« 
        new_papers = []
        for paper_id, paper in all_papers.items():
            if paper_id not in self.seen_ids:
                new_papers.append(paper)
                self.seen_ids.add(paper_id)
        
        logger.info(f"å…¶ä¸­ {len(new_papers)} ç¯‡æ˜¯æ–°æ–‡ç« ")
        
        if not new_papers:
            logger.info("æ²¡æœ‰æ–°æ–‡ç« éœ€è¦æ¨é€")
            return ""
        
        # 4. æ’åº
        sort_by_citations = self.config.get('sort_by_citations', False)
        ranked_papers = self.ranker.rank_papers(new_papers, sort_by_citations)
        
        # 5. åº”ç”¨é˜ˆå€¼è¿‡æ»¤
        min_score = self.config.get('min_score_threshold', 1.0)
        filtered_papers = [p for p in ranked_papers if p.score >= min_score]
        
        logger.info(f"åŒ¹é…å…³é”®è¯çš„æ–‡ç« : {len(ranked_papers)} ç¯‡")
        logger.info(f"é€šè¿‡é˜ˆå€¼({min_score})çš„æ–‡ç« : {len(filtered_papers)} ç¯‡")
        
        if not filtered_papers:
            logger.warning(f"æ²¡æœ‰æ–‡ç« é€šè¿‡ç›¸å…³æ€§é˜ˆå€¼({min_score})ï¼Œå°è¯•æ˜¾ç¤ºå¾—åˆ†æœ€é«˜çš„å‡ ç¯‡")
            # å¦‚æœæ²¡æœ‰é€šè¿‡é˜ˆå€¼çš„æ–‡ç« ï¼Œæ˜¾ç¤ºå¾—åˆ†æœ€é«˜çš„å‰5ç¯‡
            filtered_papers = ranked_papers[:5]
        
        # 6. è·å–å¼•ç”¨æ¬¡æ•°
        fetch_citations = self.config.get('fetch_citations', False) or sort_by_citations
        if fetch_citations and filtered_papers:
            self.citation_fetcher.batch_get_citations(filtered_papers)
            
            if sort_by_citations:
                filtered_papers.sort(key=lambda p: (-p.citation_count, -p.score))
        
        # 7. é™åˆ¶æ•°é‡
        max_papers = self.config.get('max_papers_per_day', 30)
        selected_papers = filtered_papers[:max_papers]
        
        logger.info(f"æœ€ç»ˆé€‰æ‹© {len(selected_papers)} ç¯‡æ–‡ç« ")
        
        # æ‰“å°é€‰ä¸­çš„æ–‡ç« 
        for i, paper in enumerate(selected_papers, 1):
            logger.info(f"  {i}. {paper.title[:60]}... (å¾—åˆ†: {paper.score:.1f})")
        
        # 8. ç”ŸæˆæŠ¥å‘Š
        output_path = self._generate_report(selected_papers)
        
        # 9. å‘é€é‚®ä»¶æ¨é€
        if send_email and selected_papers and self.email_sender:
            date_str = datetime.now().strftime('%Y-%m-%d')
            success = self.email_sender.send_papers_email(
                selected_papers, 
                output_path, 
                date_str
            )
            if success:
                logger.info("ğŸ“§ é‚®ä»¶æ¨é€æˆåŠŸï¼")
            else:
                logger.error("ğŸ“§ é‚®ä»¶æ¨é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        elif not self.email_sender and self.config.get('email', {}).get('enabled'):
            logger.warning("é‚®ä»¶åŠŸèƒ½å·²å¯ç”¨ä½†å‘é€å™¨æœªåˆå§‹åŒ–")
        elif not selected_papers:
            logger.warning("æ²¡æœ‰é€‰ä¸­çš„æ–‡ç« ï¼Œè·³è¿‡é‚®ä»¶å‘é€")
        
        # 10. ä¿å­˜å†å²
        self._save_history()
        
        if output_path:
            logger.info(f"ä»»åŠ¡å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        return output_path
    
    def _generate_report(self, papers: List[Paper]) -> str:
        """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
        if not papers:
            return ""
        
        output_dir = self.config.get('output_dir', 'daily_papers')
        os.makedirs(output_dir, exist_ok=True)
        
        today = datetime.now().strftime('%Y-%m-%d')
        filename = f"arxiv_papers_{today}.md"
        filepath = os.path.join(output_dir, filename)
        
        # æŒ‰ä¸»é¢˜åˆ†ç»„
        groups = {
            'äº§ä¸šç»„ç»‡ä¸å¸‚åœº': [],
            'èˆªè¿ä¸ç¯å¢ƒ': [],
            'å…¶ä»–ç›¸å…³æ–‡ç« ': []
        }
        
        for paper in papers:
            matched_text = ' '.join(paper.matched_keywords).lower()
            if any(kw in matched_text for kw in ['èˆªè¿', 'ç¢³', 'ship', 'carbon', 'arctic', 'maritime']):
                groups['èˆªè¿ä¸ç¯å¢ƒ'].append(paper)
            elif any(kw in matched_text for kw in ['å¸‚åœº', 'äº§ä¸š', 'å®šä»·', 'market', 'industr']):
                groups['äº§ä¸šç»„ç»‡ä¸å¸‚åœº'].append(paper)
            else:
                groups['å…¶ä»–ç›¸å…³æ–‡ç« '].append(paper)
        
        # ç”Ÿæˆ Markdown
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# ğŸ“š arXiv æ¯æ—¥æ–‡ç« æ¨é€ ({today})\n\n")
            f.write(f"> å…±ç­›é€‰å‡º **{len(papers)}** ç¯‡ç›¸å…³æ–‡ç« \n\n")
            
            if self.config.get('sort_by_citations', False):
                f.write("> ğŸ“Š æŒ‰ **å¼•ç”¨æ¬¡æ•°** é™åºæ’åˆ—\n\n")
            else:
                f.write("> ğŸ“Š æŒ‰ **ç›¸å…³æ€§** é™åºæ’åˆ—\n\n")
            
            f.write("---\n\n")
            
            for group_name, group_papers in groups.items():
                if not group_papers:
                    continue
                
                f.write(f"## {group_name}\n\n")
                
                for i, paper in enumerate(group_papers, 1):
                    f.write(f"### {i}. {paper.title}\n\n")
                    f.write(f"- **ä½œè€…**: {', '.join(paper.authors[:5])}")
                    if len(paper.authors) > 5:
                        f.write(f" ç­‰ ({len(paper.authors)} äºº)")
                    f.write("\n")
                    f.write(f"- **å‘å¸ƒæ—¶é—´**: {paper.published.strftime('%Y-%m-%d')}\n")
                    f.write(f"- **åˆ†ç±»**: {', '.join(paper.categories[:3])}\n")
                    f.write(f"- **ç›¸å…³æ€§å¾—åˆ†**: {paper.score:.1f}\n")
                    
                    if paper.citation_count > 0:
                        f.write(f"- **è¢«å¼•æ¬¡æ•°**: {paper.citation_count}\n")
                    
                    if paper.matched_keywords:
                        f.write(f"- **åŒ¹é…å…³é”®è¯**: {', '.join(paper.matched_keywords[:5])}\n")
                    
                    f.write(f"- **é“¾æ¥**: [arXiv]({paper.link})")
                    if paper.pdf_link:
                        f.write(f" | [PDF]({paper.pdf_link})")
                    f.write("\n\n")
                    
                    summary = paper.summary[:800]
                    if len(paper.summary) > 800:
                        summary += "..."
                    f.write(f"> **æ‘˜è¦**: {summary}\n\n")
                    f.write("---\n\n")
            
            f.write("\n*ç”± arXiv Agent è‡ªåŠ¨ç”Ÿæˆ*\n")
        
        return filepath
    
    def test_email(self) -> bool:
        """æµ‹è¯•é‚®ä»¶é…ç½®"""
        if not self.email_sender:
            logger.error("é‚®ä»¶å‘é€å™¨æœªåˆå§‹åŒ–")
            return False
        return self.email_sender.test_connection()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='arXiv æ¯æ—¥æ–‡ç« æ¨é€æ™ºèƒ½ä½“')
    parser.add_argument('--no-email', action='store_true', help='ä¸å‘é€é‚®ä»¶')
    parser.add_argument('--test-email', action='store_true', help='æµ‹è¯•é‚®ä»¶é…ç½®')
    parser.add_argument('--config', default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--sort-by-citations', action='store_true', help='æŒ‰å¼•ç”¨æ’åº')
    parser.add_argument('--fetch-citations', action='store_true', help='è·å–å¼•ç”¨æ¬¡æ•°')
    
    args = parser.parse_args()
    
    agent = ArxivAgent(config_file=args.config)
    
    if args.sort_by_citations:
        agent.config['sort_by_citations'] = True
    if args.fetch_citations:
        agent.config['fetch_citations'] = True
    
    if args.test_email:
        success = agent.test_email()
        exit(0 if success else 1)
    
    report_path = agent.run(send_email=not args.no_email)
    
    if report_path:
        print(f"\nâœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    else:
        print("\nâš ï¸ æœªç”ŸæˆæŠ¥å‘Š")
    
    if agent.email_sender and not args.no_email:
        receivers = agent.config.get('email', {}).get('receiver_emails', [])
        print(f"ğŸ“§ æ”¶ä»¶äºº: {', '.join(receivers)}")


if __name__ == "__main__":
    main()
