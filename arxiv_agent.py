#!/usr/bin/env python3
"""
arXiv æ¯æ—¥æ–‡ç« æ¨é€æ™ºèƒ½ä½“
æ ¹æ®å…³é”®è¯è‡ªåŠ¨æŠ“å– arXiv è®ºæ–‡ï¼Œæ¯å¤©æ¨é€æœ€å¤š30ç¯‡ç›¸å…³æ–‡ç« 
æ”¯æŒé‚®ä»¶æ¨é€åŠŸèƒ½ï¼Œæ”¯æŒ GitHub Actions éƒ¨ç½²
æ”¯æŒå¼•ç”¨æ¬¡æ•°æ’åº
"""

import os
import re
import yaml
import json
import logging
import feedparser
import requests
import argparse
from datetime import datetime, timedelta
from typing import List, Dict, Set, Tuple, Optional
from dataclasses import dataclass, field
from pathlib import Path

# å¯¼å…¥é‚®ä»¶å‘é€æ¨¡å—
try:
    from email_sender import EmailSender
    EMAIL_AVAILABLE = True
except ImportError:
    EMAIL_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class Paper:
    """è®ºæ–‡æ•°æ®ç»“æ„"""
    title: str
    authors: List[str]
    summary: str
    link: str
    pdf_link: str
    published: datetime
    categories: List[str]
    primary_category: str
    arxiv_id: str = ""  # arXiv ID
    score: float = 0.0  # ç›¸å…³æ€§å¾—åˆ†
    citation_count: int = 0  # å¼•ç”¨æ¬¡æ•°
    matched_keywords: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        return {
            'title': self.title,
            'authors': self.authors,
            'summary': self.summary[:500] + '...' if len(self.summary) > 500 else self.summary,
            'link': self.link,
            'pdf_link': self.pdf_link,
            'published': self.published.strftime('%Y-%m-%d'),
            'categories': self.categories,
            'primary_category': self.primary_category,
            'arxiv_id': self.arxiv_id,
            'score': round(self.score, 2),
            'citation_count': self.citation_count,
            'matched_keywords': self.matched_keywords
        }


class KeywordManager:
    """å…³é”®è¯ç®¡ç†å™¨"""
    
    def __init__(self, keywords_file: str = "keywords.txt"):
        self.keywords_file = keywords_file
        self.keywords = []
        self.keyword_groups = {}
        self.core_keywords = []  # æ ¸å¿ƒå…³é”®è¯ï¼ˆå¿…é¡»åŒ¹é…è‡³å°‘ä¸€ä¸ªï¼‰
        self.extended_keywords = []  # æ‰©å±•å…³é”®è¯ï¼ˆåŠ åˆ†é¡¹ï¼‰
        self._load_keywords()
    
    def _load_keywords(self):
        """ä»æ–‡ä»¶åŠ è½½å…³é”®è¯ï¼Œæ”¯æŒåˆ†ç»„å’Œæ ¸å¿ƒ/æ‰©å±•åŒºåˆ†"""
        if not os.path.exists(self.keywords_file):
            raise FileNotFoundError(f"å…³é”®è¯æ–‡ä»¶ä¸å­˜åœ¨: {self.keywords_file}")
        
        with open(self.keywords_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        current_group = "default"
        is_extended_section = False
        self.keyword_groups[current_group] = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æµ‹åˆ†ç»„æ ‡é¢˜
            if line.endswith('å…³é”®è¯') or 'æ‰©å±•' in line.lower():
                current_group = line
                self.keyword_groups[current_group] = []
                if 'æ‰©å±•' in line.lower():
                    is_extended_section = True
                continue
            
            # å¤„ç†ä¸€è¡Œå¤šä¸ªå…³é”®è¯çš„æƒ…å†µ
            sub_keywords = re.split(r'[\sã€,ï¼Œ]+', line)
            for kw in sub_keywords:
                kw = kw.strip().lower()
                if kw and len(kw) > 1:
                    self.keywords.append(kw)
                    self.keyword_groups[current_group].append(kw)
                    
                    # åŒºåˆ†æ ¸å¿ƒå…³é”®è¯å’Œæ‰©å±•å…³é”®è¯
                    if is_extended_section:
                        self.extended_keywords.append(kw)
                    else:
                        self.core_keywords.append(kw)
        
        # å»é‡
        self.keywords = list(set(self.keywords))
        self.core_keywords = list(set(self.core_keywords))
        self.extended_keywords = list(set(self.extended_keywords))
        
        logger.info(f"åŠ è½½äº† {len(self.keywords)} ä¸ªå…³é”®è¯")
        logger.info(f"  - æ ¸å¿ƒå…³é”®è¯: {len(self.core_keywords)} ä¸ª")
        logger.info(f"  - æ‰©å±•å…³é”®è¯: {len(self.extended_keywords)} ä¸ª")
    
    def get_search_queries(self) -> List[str]:
        """ç”Ÿæˆ arXiv æœç´¢æŸ¥è¯¢è¯"""
        translations = {
            # äº§ä¸šç»„ç»‡
            'ç©ºè°ƒå¸‚åœº': 'air conditioner market',
            'ç”µåŠ¨æ±½è½¦å¸‚åœº': 'electric vehicle market',
            'ç”µè½¦å¸‚åœº': 'EV market',
            'è€ç”¨æ¶ˆè´¹å“': 'durable goods',
            'å®è¯äº§ä¸šç»„ç»‡': 'empirical industrial organization',
            'å®è¯ io': 'empirical IO',
            'å®è¯äº§ä¸šç»„ç»‡å­¦': 'empirical industrial organization',
            'å¸‚åœºç»“æ„': 'market structure',
            'äº§å“å·®å¼‚åŒ–': 'product differentiation',
            'éœ€æ±‚ä¼°è®¡': 'demand estimation',
            'éœ€æ±‚ä¼°è®¡æ¨¡å‹': 'demand estimation',
            'ä¾›ç»™è¡Œä¸º': 'supply behavior',
            'å®šä»·ç­–ç•¥': 'pricing strategy',
            'å¸‚åœºåŠ¿åŠ›': 'market power',
            'ç¦åˆ©åˆ†æ': 'welfare analysis',
            'å®¶ç”µå¸‚åœº': 'appliance market',
            'å®¶ç”¨ç”µå™¨å¸‚åœº': 'home appliance market',
            'æ–°èƒ½æºæ±½è½¦å¸‚åœº': 'new energy vehicle market',
            'ç¦»æ•£é€‰æ‹©æ¨¡å‹': 'discrete choice model',
            'blp æ¨¡å‹': 'BLP model',
            'blp': 'BLP',
            'ç»“æ„ä¼°è®¡': 'structural estimation',
            'ç»“æ„å¼ä¼°è®¡': 'structural estimation',
            'å¯¡å¤´ç«äº‰': 'oligopoly competition',
            'å¯¡å¤´å„æ–­': 'oligopoly',
            'çºµå‘å…³ç³»': 'vertical relationship',
            'æŠ€æœ¯åˆ›æ–°': 'technological innovation',
            'æŠ€æœ¯å˜é©': 'technological change',
            'æ”¿ç­–è¯„ä¼°': 'policy evaluation',
            'æ”¿ç­–è¯„ä»·': 'policy evaluation',
            'æ¶ˆè´¹è¡Œä¸º': 'consumer behavior',
            'æ¶ˆè´¹è€…è¡Œä¸º': 'consumer behavior',
            # èˆªè¿ç›¸å…³
            'åŒ—æèˆªé“': 'Arctic shipping route',
            'åŒ—æèˆªçº¿': 'Arctic shipping route',
            'åŒ—æèˆªè¿': 'Arctic shipping',
            'å…¨çƒèˆªè¿è´¸æ˜“': 'global shipping trade',
            'å…¨çƒæµ·è¿è´¸æ˜“': 'global maritime trade',
            'æµ·è¿ç¢³æ’æ”¾': 'maritime carbon emission',
            'æµ·æ´‹ç¢³æ’æ”¾': 'maritime carbon emission',
            'èˆªè¿å‡æ’': 'shipping emission reduction',
            'èˆ¹èˆ¶ç¢³æ’æ”¾': 'vessel carbon emission',
            'èˆ¹èˆ¶æ’æ”¾': 'vessel emission',
            'ç¢³å‡æ’æ”¿ç­–': 'carbon reduction policy',
            'ç¢³æ’æ”¾æ”¿ç­–': 'carbon emission policy',
            'èˆªè¿ç¢³è¶³è¿¹': 'shipping carbon footprint',
            'ç»¿è‰²èˆªè¿': 'green shipping',
            'æ°”å€™å½±å“': 'climate impact',
            'æ°”å€™å˜åŒ–å½±å“': 'climate impact',
            'å›½é™…æµ·è¿': 'international shipping',
            'å›½é™…èˆªè¿': 'international shipping',
            'æµ·è¿è´¸æ˜“æ ¼å±€': 'maritime trade pattern',
            'èˆªè¿è´¸æ˜“': 'shipping trade',
            'ç¢³ç¨': 'carbon tax',
            'ç¢³å¸‚åœº': 'carbon market',
            'ç¢³äº¤æ˜“å¸‚åœº': 'carbon market',
            'èˆ¹èˆ¶èƒ½æ•ˆ': 'ship energy efficiency',
            'èˆ¹èˆ¶èƒ½æºæ•ˆç‡': 'ship energy efficiency',
            'ä½ç¢³èˆªè¿': 'low carbon shipping',
            'ä½ç¢³æµ·è¿': 'low carbon shipping',
            'åŒ—æç¯å¢ƒå½±å“': 'Arctic environmental impact',
            'è´¸æ˜“è·¯çº¿ä¼˜åŒ–': 'trade route optimization',
            'èˆªçº¿ä¼˜åŒ–': 'route optimization',
            'å¯æŒç»­èˆªè¿': 'sustainable shipping',
            'å¯æŒç»­æµ·è¿': 'sustainable maritime',
        }
        
        queries = []
        for kw in self.keywords:
            if kw in translations:
                queries.append(translations[kw])
            elif kw.isascii():
                queries.append(kw)
        
        return list(set(queries))


class CitationFetcher:
    """å¼•ç”¨æ¬¡æ•°è·å–å™¨ - ä½¿ç”¨ Semantic Scholar API"""
    
    API_URL = "https://api.semanticscholar.org/graph/v1/paper/"
    
    def __init__(self, timeout: int = 10):
        self.timeout = timeout
    
    def get_citation_count(self, arxiv_id: str) -> int:
        """
        è·å–è®ºæ–‡çš„å¼•ç”¨æ¬¡æ•°
        
        Args:
            arxiv_id: arXiv ID (å¦‚ 2401.12345)
            
        Returns:
            å¼•ç”¨æ¬¡æ•°ï¼Œè·å–å¤±è´¥è¿”å› 0
        """
        if not arxiv_id:
            return 0
        
        try:
            # Semantic Scholar API æ”¯æŒé€šè¿‡ arXiv ID æŸ¥è¯¢
            url = f"{self.API_URL}arXiv:{arxiv_id}"
            params = {
                'fields': 'citationCount'
            }
            
            response = requests.get(url, params=params, timeout=self.timeout)
            
            if response.status_code == 200:
                data = response.json()
                count = data.get('citationCount', 0)
                return count if count else 0
            else:
                logger.debug(f"è·å–å¼•ç”¨æ¬¡æ•°å¤±è´¥ {arxiv_id}: HTTP {response.status_code}")
                return 0
                
        except Exception as e:
            logger.debug(f"è·å–å¼•ç”¨æ¬¡æ•°å¼‚å¸¸ {arxiv_id}: {e}")
            return 0
    
    def batch_get_citations(self, papers: List[Paper], max_workers: int = 5) -> None:
        """
        æ‰¹é‡è·å–å¼•ç”¨æ¬¡æ•°
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
        """
        logger.info(f"æ­£åœ¨è·å– {len(papers)} ç¯‡è®ºæ–‡çš„å¼•ç”¨æ¬¡æ•°...")
        
        # ä¸ºäº†ç¤¼è²Œæ€§è¯·æ±‚ï¼Œä½¿ç”¨é¡ºåºè·å–è€Œä¸æ˜¯å¹¶å‘
        for i, paper in enumerate(papers):
            if paper.arxiv_id:
                paper.citation_count = self.get_citation_count(paper.arxiv_id)
                if (i + 1) % 10 == 0:
                    logger.info(f"  å·²å¤„ç† {i + 1}/{len(papers)} ç¯‡")
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                import time
                time.sleep(0.5)
        
        logger.info("å¼•ç”¨æ¬¡æ•°è·å–å®Œæˆ")


class ArxivSearcher:
    """arXiv æœç´¢å™¨"""
    
    ARXIV_API_URL = "http://export.arxiv.org/api/query"
    
    def __init__(self, max_results_per_query: int = 50):
        self.max_results_per_query = max_results_per_query
    
    def search(self, query: str, days_back: int = 7) -> List[Paper]:
        """
        æœç´¢ arXiv æ–‡ç« 
        
        Args:
            query: æœç´¢å…³é”®è¯
            days_back: æœç´¢æœ€è¿‘å‡ å¤©çš„æ–‡ç« 
        """
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        params = {
            'search_query': f'all:{query}',
            'start': 0,
            'max_results': self.max_results_per_query,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        try:
            logger.info(f"æœç´¢ arXiv: {query}")
            response = requests.get(
                self.ARXIV_API_URL, 
                params=params, 
                timeout=30
            )
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            papers = []
            
            for entry in feed.entries:
                published = datetime.strptime(
                    entry.published, 
                    '%Y-%m-%dT%H:%M:%SZ'
                )
                
                if published < start_date:
                    continue
                
                pdf_link = ""
                for link in entry.links:
                    if link.get('type') == 'application/pdf':
                        pdf_link = link.href
                        break
                
                authors = [author.name for author in entry.get('authors', [])]
                categories = [tag.term for tag in entry.get('tags', [])]
                primary_cat = entry.get('arxiv_primary_category', {}).get('term', '')
                
                # æå– arXiv ID
                arxiv_id = ""
                if '/abs/' in entry.link:
                    arxiv_id = entry.link.split('/abs/')[-1].split('v')[0]
                
                paper = Paper(
                    title=entry.title.replace('\n', ' ').strip(),
                    authors=authors,
                    summary=entry.summary.replace('\n', ' ').strip(),
                    link=entry.link,
                    pdf_link=pdf_link,
                    published=published,
                    categories=categories,
                    primary_category=primary_cat,
                    arxiv_id=arxiv_id
                )
                papers.append(paper)
            
            logger.info(f"  æ‰¾åˆ° {len(papers)} ç¯‡æ–‡ç« ")
            return papers
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥ '{query}': {e}")
            return []


class PaperRanker:
    """æ–‡ç« æ’åºå™¨ - åŸºäºç›¸å…³æ€§å’Œå¼•ç”¨æ¬¡æ•°"""
    
    CAT_PREFIXES = ['econ.', 'q-fin.', 'stat.', 'cs.']
    
    def __init__(self, keyword_manager: KeywordManager):
        self.keyword_manager = keyword_manager
    
    def calculate_score(self, paper: Paper) -> Tuple[float, List[str]]:
        """
        è®¡ç®—æ–‡ç« ç›¸å…³æ€§å¾—åˆ†
        
        è¯„åˆ†è§„åˆ™ï¼š
        1. å¿…é¡»åŒ¹é…è‡³å°‘ä¸€ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼ˆå¦åˆ™å¾—åˆ†ä¸º0ï¼Œä¼šè¢«è¿‡æ»¤ï¼‰
        2. æ ¸å¿ƒå…³é”®è¯åŒ¹é…å¾—åˆ†é«˜
        3. æ‰©å±•å…³é”®è¯åŒ¹é…é¢å¤–åŠ åˆ†
        4. åˆ†ç±»ç›¸å…³æ€§
        5. æ—¶æ•ˆæ€§
        """
        score = 0.0
        matched_keywords = []
        text = f"{paper.title} {paper.summary}".lower()
        title_lower = paper.title.lower()
        
        # 1. æ ¸å¿ƒå…³é”®è¯åŒ¹é…ï¼ˆå¿…é¡»è‡³å°‘åŒ¹é…ä¸€ä¸ªï¼‰
        has_core_match = False
        for kw in self.keyword_manager.core_keywords:
            if kw in text:
                has_core_match = True
                if kw in title_lower:
                    score += 5.0  # æ ‡é¢˜åŒ¹é…æƒé‡å¾ˆé«˜
                else:
                    score += 2.0  # æ‘˜è¦åŒ¹é…
                matched_keywords.append(kw)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…æ ¸å¿ƒå…³é”®è¯ï¼Œè¿”å›0åˆ†ï¼ˆå°†è¢«è¿‡æ»¤ï¼‰
        if not has_core_match:
            return 0.0, []
        
        # 2. æ‰©å±•å…³é”®è¯åŒ¹é…ï¼ˆé¢å¤–åŠ åˆ†ï¼‰
        for kw in self.keyword_manager.extended_keywords:
            if kw in text and kw not in matched_keywords:
                if kw in title_lower:
                    score += 2.0
                else:
                    score += 0.5
                matched_keywords.append(kw)
        
        # 3. åˆ†ç±»ç›¸å…³æ€§å¾—åˆ†
        for cat in paper.categories:
            for prefix in self.CAT_PREFIXES:
                if cat.startswith(prefix):
                    score += 0.5
                    break
        
        # 4. æ—¶æ•ˆæ€§å¾—åˆ†
        days_since_published = (datetime.now() - paper.published).days
        if days_since_published <= 1:
            score += 2.0
        elif days_since_published <= 3:
            score += 1.0
        
        return score, matched_keywords
    
    def rank_papers(self, papers: List[Paper], sort_by_citations: bool = False) -> List[Paper]:
        """
        å¯¹æ–‡ç« è¿›è¡Œæ’åº
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            sort_by_citations: æ˜¯å¦æŒ‰å¼•ç”¨æ¬¡æ•°æ’åº
        """
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        for paper in papers:
            paper.score, paper.matched_keywords = self.calculate_score(paper)
        
        # è¿‡æ»¤æ‰æ²¡æœ‰æ ¸å¿ƒå…³é”®è¯åŒ¹é…çš„æ–‡ç« 
        papers = [p for p in papers if p.score > 0]
        
        if sort_by_citations:
            # æŒ‰å¼•ç”¨æ¬¡æ•°é™åºï¼Œå¼•ç”¨æ¬¡æ•°ç›¸åŒåˆ™æŒ‰ç›¸å…³æ€§
            papers.sort(key=lambda p: (-p.citation_count, -p.score))
        else:
            # æŒ‰ç›¸å…³æ€§å¾—åˆ†é™åº
            papers.sort(key=lambda p: -p.score)
        
        return papers


def load_config_from_env() -> Dict:
    """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®ï¼ˆç”¨äº GitHub Actionsï¼‰"""
    config = {}
    
    email_enabled = os.environ.get('EMAIL_ENABLED', '').lower()
    if email_enabled in ('true', '1', 'yes'):
        config['email'] = {
            'enabled': True,
            'sender_email': os.environ.get('EMAIL_SENDER', ''),
            'sender_password': os.environ.get('EMAIL_PASSWORD', ''),
            'receiver_emails': os.environ.get('EMAIL_RECEIVERS', '').split(','),
            'smtp_host': os.environ.get('SMTP_HOST', ''),
            'smtp_port': int(os.environ.get('SMTP_PORT', '465') or '465'),
            'use_ssl': os.environ.get('USE_SSL', 'true').lower() == 'true',
            'use_tls': os.environ.get('USE_TLS', 'false').lower() == 'true',
        }
        config['email']['receiver_emails'] = [
            email.strip() for email in config['email']['receiver_emails'] 
            if email.strip()
        ]
    
    if os.environ.get('MAX_PAPERS'):
        config['max_papers_per_day'] = int(os.environ['MAX_PAPERS'])
    if os.environ.get('DAYS_BACK'):
        config['days_back'] = int(os.environ['DAYS_BACK'])
    if os.environ.get('MIN_SCORE'):
        config['min_score_threshold'] = float(os.environ['MIN_SCORE'])
    if os.environ.get('SORT_BY_CITATIONS'):
        config['sort_by_citations'] = os.environ['SORT_BY_CITATIONS'].lower() == 'true'
    
    return config


class ArxivAgent:
    """arXiv æ–‡ç« æ¨é€æ™ºèƒ½ä½“ä¸»ç±»"""
    
    def __init__(self, config_file: str = "config.yaml"):
        self.config = self._load_config(config_file)
        self.keyword_manager = KeywordManager(self.config.get('keywords_file', 'keywords.txt'))
        self.searcher = ArxivSearcher(
            max_results_per_query=self.config.get('max_results_per_query', 50)
        )
        self.ranker = PaperRanker(self.keyword_manager)
        self.citation_fetcher = CitationFetcher()
        
        # é‚®ä»¶å‘é€å™¨
        self.email_sender: Optional[EmailSender] = None
        if EMAIL_AVAILABLE and self.config.get('email', {}).get('enabled', False):
            try:
                self.email_sender = EmailSender(self.config['email'])
                logger.info("âœ… é‚®ä»¶å‘é€åŠŸèƒ½å·²å¯ç”¨")
            except Exception as e:
                logger.error(f"é‚®ä»¶å‘é€å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # å»é‡å­˜å‚¨
        self.seen_ids: Set[str] = set()
        self.history_file = self.config.get('history_file', 'paper_history.json')
        self._load_history()
    
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'keywords_file': 'keywords.txt',
            'max_results_per_query': 50,
            'max_papers_per_day': 30,
            'days_back': 7,
            'output_dir': 'daily_papers',
            'history_file': 'paper_history.json',
            'min_score_threshold': 2.0,  # æé«˜é˜ˆå€¼ï¼Œç¡®ä¿æ ¸å¿ƒå…³é”®è¯åŒ¹é…
            'sort_by_citations': False,  # é»˜è®¤ä¸æŒ‰å¼•ç”¨æ’åº
            'fetch_citations': False,    # é»˜è®¤ä¸è·å–å¼•ç”¨ï¼ˆèŠ‚çœAPIè°ƒç”¨ï¼‰
            'email': {
                'enabled': False
            }
        }
        
        # åŠ è½½ YAML é…ç½®
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                yaml_config = yaml.safe_load(f)
                if yaml_config:
                    default_config.update(yaml_config)
        
        # åŠ è½½ç¯å¢ƒå˜é‡é…ç½®ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        env_config = load_config_from_env()
        if env_config:
            logger.info("ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®")
            default_config.update(env_config)
        
        return default_config
    
    def _load_history(self):
        """åŠ è½½å·²æ¨é€æ–‡ç« å†å²"""
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
                    self.seen_ids = set(history.get('paper_ids', []))
                    logger.info(f"åŠ è½½å†å²è®°å½•: {len(self.seen_ids)} ç¯‡æ–‡ç« ")
            except Exception as e:
                logger.warning(f"åŠ è½½å†å²è®°å½•å¤±è´¥: {e}")
    
    def _save_history(self):
        """ä¿å­˜å·²æ¨é€æ–‡ç« å†å²"""
        history = {
            'paper_ids': list(self.seen_ids),
            'last_update': datetime.now().isoformat()
        }
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    
    def _get_paper_id(self, paper: Paper) -> str:
        """ç”Ÿæˆæ–‡ç« å”¯ä¸€ID"""
        if paper.arxiv_id:
            return paper.arxiv_id
        return paper.title[:50]
    
    def run(self, send_email: bool = True) -> str:
        """
        æ‰§è¡Œæ¯æ—¥æ–‡ç« æŠ“å–å’Œæ¨é€
        
        Args:
            send_email: æ˜¯å¦å‘é€é‚®ä»¶æ¨é€
            
        Returns:
            ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ‰§è¡Œ arXiv æ–‡ç« æ¨é€ä»»åŠ¡")
        logger.info("=" * 60)
        
        # 1. è·å–æ‰€æœ‰æœç´¢è¯
        queries = self.keyword_manager.get_search_queries()
        logger.info(f"æœç´¢å…³é”®è¯: {queries}")
        
        # 2. æœç´¢æ–‡ç« 
        all_papers: Dict[str, Paper] = {}
        days_back = self.config.get('days_back', 7)
        
        for query in queries:
            papers = self.searcher.search(query, days_back=days_back)
            for paper in papers:
                paper_id = self._get_paper_id(paper)
                if paper_id not in all_papers:
                    all_papers[paper_id] = paper
            
            import time
            time.sleep(1)
        
        logger.info(f"å…±æ‰¾åˆ° {len(all_papers)} ç¯‡ä¸é‡å¤æ–‡ç« ")
        
        # 3. è¿‡æ»¤å·²æ¨é€çš„æ–‡ç« 
        new_papers = []
        for paper_id, paper in all_papers.items():
            if paper_id not in self.seen_ids:
                new_papers.append(paper)
                self.seen_ids.add(paper_id)
        
        logger.info(f"å…¶ä¸­ {len(new_papers)} ç¯‡æ˜¯æ–°æ–‡ç« ")
        
        # 4. è®¡ç®—ç›¸å…³æ€§å¹¶è¿‡æ»¤
        ranked_papers = self.ranker.rank_papers(new_papers)
        logger.info(f"åŒ¹é…æ ¸å¿ƒå…³é”®è¯çš„æ–‡ç« : {len(ranked_papers)} ç¯‡")
        
        # 5. åº”ç”¨é˜ˆå€¼è¿‡æ»¤
        min_score = self.config.get('min_score_threshold', 2.0)
        filtered_papers = [p for p in ranked_papers if p.score >= min_score]
        logger.info(f"é€šè¿‡ç›¸å…³æ€§é˜ˆå€¼({min_score})çš„æ–‡ç« : {len(filtered_papers)} ç¯‡")
        
        # 6. è·å–å¼•ç”¨æ¬¡æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        sort_by_citations = self.config.get('sort_by_citations', False)
        fetch_citations = self.config.get('fetch_citations', False) or sort_by_citations
        
        if fetch_citations and filtered_papers:
            self.citation_fetcher.batch_get_citations(filtered_papers)
            
            # å¦‚æœéœ€è¦æŒ‰å¼•ç”¨æ’åºï¼Œé‡æ–°æ’åº
            if sort_by_citations:
                filtered_papers.sort(key=lambda p: (-p.citation_count, -p.score))
                logger.info("å·²æŒ‰å¼•ç”¨æ¬¡æ•°æ’åº")
        
        # 7. é™åˆ¶æ•°é‡
        max_papers = self.config.get('max_papers_per_day', 30)
        selected_papers = filtered_papers[:max_papers]
        
        logger.info(f"æœ€ç»ˆé€‰æ‹© {len(selected_papers)} ç¯‡æ–‡ç« ")
        
        # 8. ç”ŸæˆæŠ¥å‘Š
        output_path = self._generate_report(selected_papers)
        
        # 9. å‘é€é‚®ä»¶æ¨é€
        if send_email and selected_papers and self.email_sender:
            date_str = datetime.now().strftime('%Y-%m-%d')
            success = self.email_sender.send_papers_email(
                selected_papers, 
                output_path, 
                date_str
            )
            if success:
                logger.info("ğŸ“§ é‚®ä»¶æ¨é€æˆåŠŸï¼")
            else:
                logger.error("ğŸ“§ é‚®ä»¶æ¨é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        elif not self.email_sender and self.config.get('email', {}).get('enabled'):
            logger.warning("é‚®ä»¶åŠŸèƒ½å·²å¯ç”¨ä½†å‘é€å™¨æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ä¾èµ–å®‰è£…")
        
        # 10. ä¿å­˜å†å²
        self._save_history()
        
        logger.info(f"ä»»åŠ¡å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        return output_path
    
    def _generate_report(self, papers: List[Paper]) -> str:
        """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
        output_dir = self.config.get('output_dir', 'daily_papers')
        os.makedirs(output_dir, exist_ok=True)
        
        today = datetime.now().strftime('%Y-%m-%d')
        filename = f"arxiv_papers_{today}.md"
        filepath = os.path.join(output_dir, filename)
        
        # æŒ‰ä¸»é¢˜åˆ†ç»„
        groups = {
            'äº§ä¸šç»„ç»‡ä¸å¸‚åœº': [],
            'èˆªè¿ä¸ç¯å¢ƒ': [],
            'å…¶ä»–ç›¸å…³æ–‡ç« ': []
        }
        
        for paper in papers:
            matched_text = ' '.join(paper.matched_keywords).lower()
            if any(kw in matched_text for kw in ['èˆªè¿', 'ç¢³', 'ship', 'carbon', 'arctic', 'maritime', 'green']):
                groups['èˆªè¿ä¸ç¯å¢ƒ'].append(paper)
            elif any(kw in matched_text for kw in ['å¸‚åœº', 'äº§ä¸š', 'ç«äº‰', 'å®šä»·', 'market', 'industr', 'competition', 'éœ€æ±‚', 'ä¾›ç»™']):
                groups['äº§ä¸šç»„ç»‡ä¸å¸‚åœº'].append(paper)
            else:
                groups['å…¶ä»–ç›¸å…³æ–‡ç« '].append(paper)
        
        # ç”Ÿæˆ Markdown
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# ğŸ“š arXiv æ¯æ—¥æ–‡ç« æ¨é€ ({today})\n\n")
            f.write(f"> å…±ç­›é€‰å‡º **{len(papers)}** ç¯‡ç›¸å…³æ–‡ç« \n\n")
            
            # æ·»åŠ æ’åºè¯´æ˜
            if self.config.get('sort_by_citations', False):
                f.write("> ğŸ“Š æŒ‰ **å¼•ç”¨æ¬¡æ•°** é™åºæ’åˆ—\n\n")
            else:
                f.write("> ğŸ“Š æŒ‰ **ç›¸å…³æ€§** é™åºæ’åˆ—\n\n")
            
            f.write("---\n\n")
            
            for group_name, group_papers in groups.items():
                if not group_papers:
                    continue
                
                f.write(f"## {group_name}\n\n")
                
                for i, paper in enumerate(group_papers, 1):
                    f.write(f"### {i}. {paper.title}\n\n")
                    f.write(f"- **ä½œè€…**: {', '.join(paper.authors[:5])}")
                    if len(paper.authors) > 5:
                        f.write(f" ç­‰ ({len(paper.authors)} äºº)")
                    f.write("\n")
                    f.write(f"- **å‘å¸ƒæ—¶é—´**: {paper.published.strftime('%Y-%m-%d')}\n")
                    f.write(f"- **åˆ†ç±»**: {', '.join(paper.categories[:3])}\n")
                    f.write(f"- **ç›¸å…³æ€§å¾—åˆ†**: {paper.score:.1f}\n")
                    
                    # æ˜¾ç¤ºå¼•ç”¨æ¬¡æ•°
                    if paper.citation_count > 0:
                        f.write(f"- **è¢«å¼•æ¬¡æ•°**: {paper.citation_count}\n")
                    
                    if paper.matched_keywords:
                        f.write(f"- **åŒ¹é…å…³é”®è¯**: {', '.join(paper.matched_keywords[:5])}\n")
                    f.write(f"- **é“¾æ¥**: [arXiv]({paper.link})")
                    if paper.pdf_link:
                        f.write(f" | [PDF]({paper.pdf_link})")
                    f.write("\n\n")
                    
                    # æ‘˜è¦
                    summary = paper.summary[:800]
                    if len(paper.summary) > 800:
                        summary += "..."
                    f.write(f"> **æ‘˜è¦**: {summary}\n\n")
                    f.write("---\n\n")
            
            # é¡µè„š
            f.write("\n*ç”± arXiv Agent è‡ªåŠ¨ç”Ÿæˆ*\n")
        
        return filepath
    
    def test_email(self) -> bool:
        """æµ‹è¯•é‚®ä»¶é…ç½®"""
        if not self.email_sender:
            logger.error("é‚®ä»¶å‘é€å™¨æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ config.yaml ä¸­çš„ email é…ç½®")
            return False
        return self.email_sender.test_connection()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='arXiv æ¯æ—¥æ–‡ç« æ¨é€æ™ºèƒ½ä½“')
    parser.add_argument(
        '--no-email',
        action='store_true',
        help='ä¸å‘é€é‚®ä»¶æ¨é€ï¼ˆä»…ç”Ÿæˆæœ¬åœ°æŠ¥å‘Šï¼‰'
    )
    parser.add_argument(
        '--test-email',
        action='store_true',
        help='æµ‹è¯•é‚®ä»¶é…ç½®'
    )
    parser.add_argument(
        '--config',
        default='config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: config.yamlï¼‰'
    )
    parser.add_argument(
        '--sort-by-citations',
        action='store_true',
        help='æŒ‰å¼•ç”¨æ¬¡æ•°æ’åº'
    )
    parser.add_argument(
        '--fetch-citations',
        action='store_true',
        help='è·å–å¼•ç”¨æ¬¡æ•°ï¼ˆä¼šå¢åŠ è¿è¡Œæ—¶é—´ï¼‰'
    )
    
    args = parser.parse_args()
    
    agent = ArxivAgent(config_file=args.config)
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.sort_by_citations:
        agent.config['sort_by_citations'] = True
    if args.fetch_citations:
        agent.config['fetch_citations'] = True
    
    if args.test_email:
        success = agent.test_email()
        exit(0 if success else 1)
    
    report_path = agent.run(send_email=not args.no_email)
    print(f"\nâœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    if agent.email_sender and not args.no_email:
        print("ğŸ“§ é‚®ä»¶å·²å‘é€è‡³:", ', '.join(agent.config.get('email', {}).get('receiver_emails', [])))


if __name__ == "__main__":
    main()
