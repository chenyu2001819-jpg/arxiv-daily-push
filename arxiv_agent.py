#!/usr/bin/env python3
"""
arXiv æ¯æ—¥æ–‡ç« æ¨é€æ™ºèƒ½ä½“
åˆ†å—ç­›é€‰ç­–ç•¥ï¼š
- ä¸¤å¤§ä¸»é¢˜ï¼ˆäº§ä¸šç»„ç»‡ã€èˆªè¿ç¯å¢ƒï¼‰
- æ¯ä¸»é¢˜åˆ†æ ¸å¿ƒå…³é”®è¯ï¼ˆå¼•ç”¨å‰Nç¯‡ï¼‰å’Œæ‰©å±•å…³é”®è¯ï¼ˆå¼•ç”¨å‰Mç¯‡ï¼‰
- æ•°é‡å¯é…ç½®
"""

import os
import re
import yaml
import json
import logging
import feedparser
import requests
import argparse
from datetime import datetime, timedelta
from typing import List, Dict, Set, Tuple, Optional
from dataclasses import dataclass, field
from pathlib import Path
from collections import defaultdict

# å¯¼å…¥é‚®ä»¶å‘é€æ¨¡å—
try:
    from email_sender import EmailSender
    EMAIL_AVAILABLE = True
except ImportError:
    EMAIL_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class Paper:
    """è®ºæ–‡æ•°æ®ç»“æ„"""
    title: str
    authors: List[str]
    summary: str
    link: str
    pdf_link: str
    published: datetime
    categories: List[str]
    primary_category: str
    arxiv_id: str = ""
    citation_count: int = 0
    matched_keywords: List[str] = field(default_factory=list)
    source_block: str = ""  # æ¥æºä¸»é¢˜å—
    keyword_type: str = ""  # core æˆ– extended
    
    def to_dict(self) -> Dict:
        return {
            'title': self.title,
            'authors': self.authors,
            'summary': self.summary[:500] + '...' if len(self.summary) > 500 else self.summary,
            'link': self.link,
            'pdf_link': self.pdf_link,
            'published': self.published.strftime('%Y-%m-%d'),
            'categories': self.categories,
            'primary_category': self.primary_category,
            'arxiv_id': self.arxiv_id,
            'citation_count': self.citation_count,
            'matched_keywords': self.matched_keywords,
            'source_block': self.source_block,
            'keyword_type': self.keyword_type
        }


class KeywordBlock:
    """å…³é”®è¯å— - ä»£è¡¨ä¸€ä¸ªä¸»é¢˜é¢†åŸŸ"""
    
    def __init__(self, name: str, core_keywords: List[str], extended_keywords: List[str]):
        self.name = name
        self.core_keywords = core_keywords
        self.extended_keywords = extended_keywords
        self.all_keywords = core_keywords + extended_keywords
        
        # ç”Ÿæˆæœç´¢æŸ¥è¯¢
        self.search_queries = self._generate_queries()
    
    def _generate_queries(self) -> List[str]:
        """ç”Ÿæˆè‹±æ–‡æœç´¢æŸ¥è¯¢"""
        translations = {
            # äº§ä¸šç»„ç»‡
            'ç©ºè°ƒå¸‚åœº': 'air conditioner market',
            'ç”µåŠ¨æ±½è½¦å¸‚åœº': 'electric vehicle market',
            'ç”µè½¦å¸‚åœº': 'EV market',
            'è€ç”¨æ¶ˆè´¹å“': 'durable goods',
            'å®è¯äº§ä¸šç»„ç»‡': 'empirical industrial organization',
            'å®è¯ io': 'empirical IO',
            'å®è¯äº§ä¸šç»„ç»‡å­¦': 'empirical industrial organization',
            'å¸‚åœºç»“æ„': 'market structure',
            'äº§å“å·®å¼‚åŒ–': 'product differentiation',
            'éœ€æ±‚ä¼°è®¡': 'demand estimation',
            'éœ€æ±‚ä¼°è®¡æ¨¡å‹': 'demand estimation',
            'ä¾›ç»™è¡Œä¸º': 'supply behavior',
            'å®šä»·ç­–ç•¥': 'pricing strategy',
            'å¸‚åœºåŠ¿åŠ›': 'market power',
            'ç¦åˆ©åˆ†æ': 'welfare analysis',
            'å®¶ç”µå¸‚åœº': 'appliance market',
            'å®¶ç”¨ç”µå™¨å¸‚åœº': 'home appliance market',
            'æ–°èƒ½æºæ±½è½¦å¸‚åœº': 'new energy vehicle market',
            'ç¦»æ•£é€‰æ‹©æ¨¡å‹': 'discrete choice model',
            'blp æ¨¡å‹': 'BLP model',
            'blp': 'BLP',
            'ç»“æ„ä¼°è®¡': 'structural estimation',
            'ç»“æ„å¼ä¼°è®¡': 'structural estimation',
            'å¯¡å¤´ç«äº‰': 'oligopoly competition',
            'å¯¡å¤´å„æ–­': 'oligopoly',
            'çºµå‘å…³ç³»': 'vertical relationship',
            'æŠ€æœ¯åˆ›æ–°': 'technological innovation',
            'æŠ€æœ¯å˜é©': 'technological change',
            'æ”¿ç­–è¯„ä¼°': 'policy evaluation',
            'æ”¿ç­–è¯„ä»·': 'policy evaluation',
            'æ¶ˆè´¹è¡Œä¸º': 'consumer behavior',
            'æ¶ˆè´¹è€…è¡Œä¸º': 'consumer behavior',
            # èˆªè¿ç›¸å…³
            'åŒ—æèˆªé“': 'Arctic shipping',
            'åŒ—æèˆªçº¿': 'Arctic shipping route',
            'åŒ—æèˆªè¿': 'Arctic shipping',
            'å…¨çƒèˆªè¿è´¸æ˜“': 'global shipping trade',
            'å…¨çƒæµ·è¿è´¸æ˜“': 'global maritime trade',
            'æµ·è¿ç¢³æ’æ”¾': 'maritime carbon emission',
            'æµ·æ´‹ç¢³æ’æ”¾': 'maritime carbon emission',
            'èˆªè¿å‡æ’': 'shipping emission reduction',
            'èˆ¹èˆ¶ç¢³æ’æ”¾': 'vessel carbon emission',
            'èˆ¹èˆ¶æ’æ”¾': 'vessel emission',
            'ç¢³å‡æ’æ”¿ç­–': 'carbon reduction policy',
            'ç¢³æ’æ”¾æ”¿ç­–': 'carbon emission policy',
            'èˆªè¿ç¢³è¶³è¿¹': 'shipping carbon footprint',
            'ç»¿è‰²èˆªè¿': 'green shipping',
            'æ°”å€™å½±å“': 'climate impact',
            'æ°”å€™å˜åŒ–å½±å“': 'climate impact',
            'å›½é™…æµ·è¿': 'international shipping',
            'å›½é™…èˆªè¿': 'international shipping',
            'æµ·è¿è´¸æ˜“æ ¼å±€': 'maritime trade pattern',
            'èˆªè¿è´¸æ˜“': 'shipping trade',
            'ç¢³ç¨': 'carbon tax',
            'ç¢³å¸‚åœº': 'carbon market',
            'ç¢³äº¤æ˜“å¸‚åœº': 'carbon market',
            'èˆ¹èˆ¶èƒ½æ•ˆ': 'ship energy efficiency',
            'èˆ¹èˆ¶èƒ½æºæ•ˆç‡': 'ship energy efficiency',
            'ä½ç¢³èˆªè¿': 'low carbon shipping',
            'ä½ç¢³æµ·è¿': 'low carbon shipping',
            'åŒ—æç¯å¢ƒå½±å“': 'Arctic environmental impact',
            'è´¸æ˜“è·¯çº¿ä¼˜åŒ–': 'trade route optimization',
            'èˆªçº¿ä¼˜åŒ–': 'route optimization',
            'å¯æŒç»­èˆªè¿': 'sustainable shipping',
            'å¯æŒç»­æµ·è¿': 'sustainable maritime',
        }
        
        queries = set()
        for kw in self.all_keywords:
            kw_lower = kw.lower()
            if kw_lower.isascii():
                queries.add(kw_lower)
            elif kw in translations:
                queries.add(translations[kw])
        
        return list(queries) if queries else ['industrial organization', 'market structure']


class KeywordManager:
    """å…³é”®è¯ç®¡ç†å™¨ - ç®¡ç†å¤šä¸ªä¸»é¢˜å—"""
    
    def __init__(self, keywords_file: str = "keywords.txt"):
        self.keywords_file = keywords_file
        self.blocks: List[KeywordBlock] = []
        self._load_keywords()
    
    def _load_keywords(self):
        """ä»æ–‡ä»¶åŠ è½½å…³é”®è¯ï¼Œåˆ†æˆå¤šä¸ªå—"""
        if not os.path.exists(self.keywords_file):
            raise FileNotFoundError(f"å…³é”®è¯æ–‡ä»¶ä¸å­˜åœ¨: {self.keywords_file}")
        
        with open(self.keywords_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ†å‰²æˆå—ï¼ˆç”¨ç©ºè¡Œåˆ†éš”ï¼‰
        raw_blocks = re.split(r'\n\s*\n', content.strip())
        
        for raw_block in raw_blocks:
            lines = [line.strip() for line in raw_block.strip().split('\n') if line.strip()]
            if not lines:
                continue
            
            # ç¬¬ä¸€è¡Œæ˜¯å—åç§°ï¼ˆå¦‚æœä¸æ˜¯å…³é”®è¯è¡Œï¼‰
            block_name = lines[0]
            if 'å…³é”®è¯' in block_name or 'æ‰©å±•' in block_name:
                block_name = f"ä¸»é¢˜å—{len(self.blocks) + 1}"
                keyword_lines = lines
            else:
                keyword_lines = lines[1:]
            
            # åˆ†ç¦»æ ¸å¿ƒå…³é”®è¯å’Œæ‰©å±•å…³é”®è¯
            core_keywords = []
            extended_keywords = []
            is_extended = False
            
            for line in keyword_lines:
                if 'å…³é”®è¯' in line.lower() or line.endswith('å…³é”®è¯'):
                    continue
                if 'æ‰©å±•' in line.lower():
                    is_extended = True
                    continue
                
                # åˆ†å‰²ä¸€è¡Œä¸­çš„å¤šä¸ªå…³é”®è¯
                sub_keywords = re.split(r'[\sã€,ï¼Œ]+', line)
                for kw in sub_keywords:
                    kw = kw.strip().lower()
                    if kw and len(kw) > 1:
                        if is_extended:
                            extended_keywords.append(kw)
                        else:
                            core_keywords.append(kw)
            
            if core_keywords or extended_keywords:
                block = KeywordBlock(block_name, core_keywords, extended_keywords)
                self.blocks.append(block)
                logger.info(f"åŠ è½½ä¸»é¢˜å— '{block_name}': {len(core_keywords)} æ ¸å¿ƒ, {len(extended_keywords)} æ‰©å±•")
                logger.info(f"  æœç´¢æŸ¥è¯¢: {block.search_queries}")
        
        if not self.blocks:
            # é»˜è®¤åˆ›å»ºä¸¤ä¸ªå—
            logger.warning("æœªæ‰¾åˆ°å…³é”®è¯å—ï¼Œåˆ›å»ºé»˜è®¤å—")
            self.blocks = [
                KeywordBlock("äº§ä¸šç»„ç»‡", ['market structure', 'industrial organization'], ['pricing']),
                KeywordBlock("èˆªè¿ç¯å¢ƒ", ['shipping', 'carbon emission'], ['maritime'])
            ]


class CitationFetcher:
    """å¼•ç”¨æ¬¡æ•°è·å–å™¨"""
    
    API_URL = "https://api.semanticscholar.org/graph/v1/paper/"
    
    def get_citation_count(self, arxiv_id: str) -> int:
        """è·å–è®ºæ–‡çš„å¼•ç”¨æ¬¡æ•°"""
        if not arxiv_id:
            return 0
        
        try:
            url = f"{self.API_URL}arXiv:{arxiv_id}"
            params = {'fields': 'citationCount'}
            
            response = requests.get(url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                return data.get('citationCount', 0) or 0
            return 0
                
        except Exception:
            return 0
    
    def batch_get_citations(self, papers: List[Paper]) -> None:
        """æ‰¹é‡è·å–å¼•ç”¨æ¬¡æ•°"""
        logger.info(f"æ­£åœ¨è·å– {len(papers)} ç¯‡è®ºæ–‡çš„å¼•ç”¨æ¬¡æ•°...")
        
        for i, paper in enumerate(papers):
            if paper.arxiv_id:
                paper.citation_count = self.get_citation_count(paper.arxiv_id)
                if (i + 1) % 10 == 0:
                    logger.info(f"  å·²å¤„ç† {i + 1}/{len(papers)} ç¯‡")
                import time
                time.sleep(0.3)
        
        logger.info("å¼•ç”¨æ¬¡æ•°è·å–å®Œæˆ")


class ArxivSearcher:
    """arXiv æœç´¢å™¨"""
    
    ARXIV_API_URL = "http://export.arxiv.org/api/query"
    
    def __init__(self, max_results_per_query: int = 100):
        self.max_results_per_query = max_results_per_query
    
    def search(self, query: str, days_back: int = 30) -> List[Paper]:
        """æœç´¢ arXiv æ–‡ç« """
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        params = {
            'search_query': f'all:{query}',
            'start': 0,
            'max_results': self.max_results_per_query,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        try:
            logger.info(f"æœç´¢ arXiv: {query}")
            response = requests.get(
                self.ARXIV_API_URL, 
                params=params, 
                timeout=30
            )
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            papers = []
            
            for entry in feed.entries:
                published = datetime.strptime(
                    entry.published, 
                    '%Y-%m-%dT%H:%M:%SZ'
                )
                
                if published < start_date:
                    continue
                
                pdf_link = ""
                for link in entry.links:
                    if link.get('type') == 'application/pdf':
                        pdf_link = link.href
                        break
                
                authors = [author.name for author in entry.get('authors', [])]
                categories = [tag.term for tag in entry.get('tags', [])]
                primary_cat = entry.get('arxiv_primary_category', {}).get('term', '')
                
                arxiv_id = ""
                if '/abs/' in entry.link:
                    arxiv_id = entry.link.split('/abs/')[-1].split('v')[0]
                
                paper = Paper(
                    title=entry.title.replace('\n', ' ').strip(),
                    authors=authors,
                    summary=entry.summary.replace('\n', ' ').strip(),
                    link=entry.link,
                    pdf_link=pdf_link,
                    published=published,
                    categories=categories,
                    primary_category=primary_cat,
                    arxiv_id=arxiv_id
                )
                papers.append(paper)
            
            logger.info(f"  æ‰¾åˆ° {len(papers)} ç¯‡æ–‡ç« ")
            return papers
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥ '{query}': {e}")
            return []


class ArxivAgent:
    """arXiv æ–‡ç« æ¨é€æ™ºèƒ½ä½“ä¸»ç±»"""
    
    def __init__(self, config_file: str = "config.yaml"):
        self.config = self._load_config(config_file)
        self.keyword_manager = KeywordManager(self.config.get('keywords_file', 'keywords.txt'))
        self.searcher = ArxivSearcher(
            max_results_per_query=self.config.get('max_results_per_query', 100)
        )
        self.citation_fetcher = CitationFetcher()
        
        # é‚®ä»¶å‘é€å™¨
        self.email_sender: Optional[EmailSender] = None
        if EMAIL_AVAILABLE and self.config.get('email', {}).get('enabled', False):
            try:
                self.email_sender = EmailSender(self.config['email'])
                logger.info("âœ… é‚®ä»¶å‘é€åŠŸèƒ½å·²å¯ç”¨")
            except Exception as e:
                logger.error(f"é‚®ä»¶å‘é€å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # å»é‡å­˜å‚¨
        self.seen_ids: Set[str] = set()
        self.history_file = self.config.get('history_file', 'paper_history.json')
        self._load_history()
    
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'keywords_file': 'keywords.txt',
            'max_results_per_query': 100,
            'days_back': 30,  # æœç´¢æœ€è¿‘30å¤©çš„æ–‡ç« 
            'output_dir': 'daily_papers',
            'history_file': 'paper_history.json',
            'email': {'enabled': False},
            # åˆ†å—ç­›é€‰é…ç½®
            'block_config': {
                'core_limit': 30,      # æ¯å—æ ¸å¿ƒå…³é”®è¯å–å‰30ç¯‡
                'extended_limit': 10,  # æ¯å—æ‰©å±•å…³é”®è¯å–å‰10ç¯‡
            }
        }
        
        # åŠ è½½ YAML é…ç½®
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                yaml_config = yaml.safe_load(f)
                if yaml_config:
                    default_config.update(yaml_config)
        
        # åŠ è½½ç¯å¢ƒå˜é‡é…ç½®
        env_config = self._load_config_from_env()
        if env_config:
            default_config.update(env_config)
        
        return default_config
    
    def _load_config_from_env(self) -> Dict:
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        config = {}
        
        email_enabled = os.environ.get('EMAIL_ENABLED', '').lower()
        if email_enabled in ('true', '1', 'yes'):
            config['email'] = {
                'enabled': True,
                'sender_email': os.environ.get('EMAIL_SENDER', ''),
                'sender_password': os.environ.get('EMAIL_PASSWORD', ''),
                'receiver_emails': [
                    e.strip() for e in os.environ.get('EMAIL_RECEIVERS', '').split(',')
                    if e.strip()
                ],
            }
        
        if os.environ.get('DAYS_BACK'):
            config['days_back'] = int(os.environ['DAYS_BACK'])
        
        # åˆ†å—é…ç½®
        block_config = {}
        if os.environ.get('CORE_LIMIT'):
            block_config['core_limit'] = int(os.environ['CORE_LIMIT'])
        if os.environ.get('EXTENDED_LIMIT'):
            block_config['extended_limit'] = int(os.environ['EXTENDED_LIMIT'])
        if block_config:
            config['block_config'] = block_config
        
        return config
    
    def _load_history(self):
        """åŠ è½½å·²æ¨é€æ–‡ç« å†å²"""
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
                    self.seen_ids = set(history.get('paper_ids', []))
                    logger.info(f"åŠ è½½å†å²è®°å½•: {len(self.seen_ids)} ç¯‡æ–‡ç« ")
            except Exception as e:
                logger.warning(f"åŠ è½½å†å²è®°å½•å¤±è´¥: {e}")
    
    def _save_history(self):
        """ä¿å­˜å·²æ¨é€æ–‡ç« å†å²"""
        history = {
            'paper_ids': list(self.seen_ids),
            'last_update': datetime.now().isoformat()
        }
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    
    def _get_paper_id(self, paper: Paper) -> str:
        """ç”Ÿæˆæ–‡ç« å”¯ä¸€ID"""
        return paper.arxiv_id if paper.arxiv_id else paper.title[:50]
    
    def _keyword_match_score(self, text: str, keywords: List[str]) -> float:
        """è®¡ç®—æ–‡æœ¬ä¸å…³é”®è¯çš„åŒ¹é…åˆ†æ•°"""
        text_lower = text.lower()
        score = 0
        for kw in keywords:
            kw_lower = kw.lower()
            if kw_lower in text_lower:
                # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
                score += 2 if text_lower.startswith(kw_lower) else 1
        return score
    
    def run(self, send_email: bool = True) -> str:
        """æ‰§è¡Œæ¯æ—¥æ–‡ç« æŠ“å–å’Œæ¨é€"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ‰§è¡Œ arXiv æ–‡ç« æ¨é€ä»»åŠ¡")
        logger.info("=" * 60)
        
        block_config = self.config.get('block_config', {})
        core_limit = block_config.get('core_limit', 30)
        extended_limit = block_config.get('extended_limit', 10)
        days_back = self.config.get('days_back', 30)
        
        logger.info(f"é…ç½®ï¼šæ ¸å¿ƒå…³é”®è¯å‰{core_limit}ç¯‡ï¼Œæ‰©å±•å…³é”®è¯å‰{extended_limit}ç¯‡")
        
        all_selected_papers: List[Paper] = []
        
        # å¯¹æ¯ä¸ªä¸»é¢˜å—è¿›è¡Œå¤„ç†
        for block in self.keyword_manager.blocks:
            logger.info(f"\nå¤„ç†ä¸»é¢˜å—: {block.name}")
            logger.info(f"  æ ¸å¿ƒå…³é”®è¯: {block.core_keywords}")
            logger.info(f"  æ‰©å±•å…³é”®è¯: {block.extended_keywords}")
            
            block_papers: List[Paper] = []
            
            # æœç´¢è¯¥ä¸»é¢˜çš„æ‰€æœ‰å…³é”®è¯
            for query in block.search_queries:
                papers = self.searcher.search(query, days_back=days_back)
                for paper in papers:
                    paper_id = self._get_paper_id(paper)
                    if paper_id not in self.seen_ids:
                        paper.source_block = block.name
                        block_papers.append(paper)
                        self.seen_ids.add(paper_id)
                import time
                time.sleep(1)
            
            logger.info(f"  æ‰¾åˆ° {len(block_papers)} ç¯‡æ–°æ–‡ç« ")
            
            if not block_papers:
                continue
            
            # è·å–å¼•ç”¨æ¬¡æ•°
            self.citation_fetcher.batch_get_citations(block_papers)
            
            # æŒ‰å¼•ç”¨æ¬¡æ•°æ’åº
            block_papers.sort(key=lambda p: -p.citation_count)
            
            # åˆ†ç±»ï¼šæ ¸å¿ƒå…³é”®è¯åŒ¹é… vs æ‰©å±•å…³é”®è¯åŒ¹é…
            core_papers = []
            extended_papers = []
            
            for paper in block_papers:
                title_summary = paper.title + " " + paper.summary
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ ¸å¿ƒå…³é”®è¯
                core_score = self._keyword_match_score(title_summary, block.core_keywords)
                if core_score > 0:
                    paper.matched_keywords = [kw for kw in block.core_keywords 
                                              if kw.lower() in title_summary.lower()]
                    paper.keyword_type = "core"
                    core_papers.append(paper)
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ‰©å±•å…³é”®è¯
                ext_score = self._keyword_match_score(title_summary, block.extended_keywords)
                if ext_score > 0:
                    paper.matched_keywords = [kw for kw in block.extended_keywords 
                                              if kw.lower() in title_summary.lower()]
                    paper.keyword_type = "extended"
                    extended_papers.append(paper)
            
            logger.info(f"  æ ¸å¿ƒå…³é”®è¯åŒ¹é…: {len(core_papers)} ç¯‡")
            logger.info(f"  æ‰©å±•å…³é”®è¯åŒ¹é…: {len(extended_papers)} ç¯‡")
            
            # é€‰å–å‰Nç¯‡
            selected_core = core_papers[:core_limit]
            selected_extended = extended_papers[:extended_limit]
            
            logger.info(f"  é€‰å–æ ¸å¿ƒæ–‡ç« : {len(selected_core)} ç¯‡")
            logger.info(f"  é€‰å–æ‰©å±•æ–‡ç« : {len(selected_extended)} ç¯‡")
            
            # åˆå¹¶è¯¥ä¸»é¢˜çš„æ–‡ç« 
            block_selected = selected_core + selected_extended
            all_selected_papers.extend(block_selected)
        
        logger.info(f"\næ€»å…±é€‰å– {len(all_selected_papers)} ç¯‡æ–‡ç« ")
        
        if not all_selected_papers:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ")
            return ""
        
        # æŒ‰ä¸»é¢˜å’Œå¼•ç”¨æ¬¡æ•°æ’åº
        all_selected_papers.sort(key=lambda p: (p.source_block, -p.citation_count))
        
        # æ‰“å°é€‰ä¸­çš„æ–‡ç« 
        for i, paper in enumerate(all_selected_papers, 1):
            logger.info(f"  {i}. [{paper.source_block}/{paper.keyword_type}] "
                       f"{paper.title[:50]}... (å¼•ç”¨: {paper.citation_count})")
        
        # ç”ŸæˆæŠ¥å‘Š
        output_path = self._generate_report(all_selected_papers)
        
        # å‘é€é‚®ä»¶
        if send_email and all_selected_papers and self.email_sender:
            date_str = datetime.now().strftime('%Y-%m-%d')
            success = self.email_sender.send_papers_email(
                all_selected_papers, output_path, date_str
            )
            if success:
                logger.info("ğŸ“§ é‚®ä»¶æ¨é€æˆåŠŸï¼")
            else:
                logger.error("ğŸ“§ é‚®ä»¶æ¨é€å¤±è´¥")
        
        # ä¿å­˜å†å²
        self._save_history()
        
        if output_path:
            logger.info(f"ä»»åŠ¡å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        return output_path
    
    def _generate_report(self, papers: List[Paper]) -> str:
        """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
        if not papers:
            return ""
        
        output_dir = self.config.get('output_dir', 'daily_papers')
        os.makedirs(output_dir, exist_ok=True)
        
        today = datetime.now().strftime('%Y-%m-%d')
        filename = f"arxiv_papers_{today}.md"
        filepath = os.path.join(output_dir, filename)
        
        # æŒ‰ä¸»é¢˜å—åˆ†ç»„
        block_groups = defaultdict(list)
        for paper in papers:
            block_groups[paper.source_block].append(paper)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# ğŸ“š arXiv æ¯æ—¥æ–‡ç« æ¨é€ ({today})\n\n")
            f.write(f"> å…±ç­›é€‰å‡º **{len(papers)}** ç¯‡ç›¸å…³æ–‡ç« \n\n")
            f.write("> ğŸ“Š æŒ‰ **å¼•ç”¨æ¬¡æ•°** é™åºæ’åˆ—\n\n")
            f.write("---\n\n")
            
            # æ±‡æ€»ç»Ÿè®¡
            f.write("## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ\n\n")
            for block_name, block_papers in block_groups.items():
                core_count = sum(1 for p in block_papers if p.keyword_type == 'core')
                ext_count = sum(1 for p in block_papers if p.keyword_type == 'extended')
                f.write(f"- **{block_name}**: {len(block_papers)} ç¯‡")
                f.write(f" (æ ¸å¿ƒ: {core_count}, æ‰©å±•: {ext_count})\n")
            f.write("\n---\n\n")
            
            # è¯¦ç»†åˆ—è¡¨
            for block_name, block_papers in block_groups.items():
                f.write(f"## {block_name}\n\n")
                
                # å†æŒ‰æ ¸å¿ƒ/æ‰©å±•åˆ†ç»„
                core_papers = [p for p in block_papers if p.keyword_type == 'core']
                ext_papers = [p for p in block_papers if p.keyword_type == 'extended']
                
                if core_papers:
                    f.write(f"### æ ¸å¿ƒå…³é”®è¯åŒ¹é… ({len(core_papers)}ç¯‡)\n\n")
                    self._write_paper_list(f, core_papers)
                
                if ext_papers:
                    f.write(f"### æ‰©å±•å…³é”®è¯åŒ¹é… ({len(ext_papers)}ç¯‡)\n\n")
                    self._write_paper_list(f, ext_papers)
            
            f.write("\n*ç”± arXiv Agent è‡ªåŠ¨ç”Ÿæˆ*\n")
        
        return filepath
    
    def _write_paper_list(self, f, papers: List[Paper]):
        """å†™å…¥è®ºæ–‡åˆ—è¡¨"""
        for i, paper in enumerate(papers, 1):
            f.write(f"#### {i}. {paper.title}\n\n")
            f.write(f"- **ä½œè€…**: {', '.join(paper.authors[:5])}")
            if len(paper.authors) > 5:
                f.write(f" ç­‰ ({len(paper.authors)} äºº)")
            f.write("\n")
            f.write(f"- **å‘å¸ƒæ—¶é—´**: {paper.published.strftime('%Y-%m-%d')}\n")
            f.write(f"- **åˆ†ç±»**: {paper.primary_category}\n")
            f.write(f"- **è¢«å¼•æ¬¡æ•°**: {paper.citation_count}\n")
            if paper.matched_keywords:
                f.write(f"- **åŒ¹é…å…³é”®è¯**: {', '.join(paper.matched_keywords[:5])}\n")
            f.write(f"- **é“¾æ¥**: [arXiv]({paper.link})")
            if paper.pdf_link:
                f.write(f" | [PDF]({paper.pdf_link})")
            f.write("\n\n")
            
            summary = paper.summary[:600]
            if len(paper.summary) > 600:
                summary += "..."
            f.write(f"> **æ‘˜è¦**: {summary}\n\n")
            f.write("---\n\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='arXiv æ¯æ—¥æ–‡ç« æ¨é€æ™ºèƒ½ä½“')
    parser.add_argument('--no-email', action='store_true', help='ä¸å‘é€é‚®ä»¶')
    parser.add_argument('--test-email', action='store_true', help='æµ‹è¯•é‚®ä»¶é…ç½®')
    parser.add_argument('--config', default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--core-limit', type=int, default=30, help='æ ¸å¿ƒå…³é”®è¯é€‰å–æ•°é‡')
    parser.add_argument('--extended-limit', type=int, default=10, help='æ‰©å±•å…³é”®è¯é€‰å–æ•°é‡')
    
    args = parser.parse_args()
    
    agent = ArxivAgent(config_file=args.config)
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.core_limit:
        agent.config.setdefault('block_config', {})['core_limit'] = args.core_limit
    if args.extended_limit:
        agent.config.setdefault('block_config', {})['extended_limit'] = args.extended_limit
    
    report_path = agent.run(send_email=not args.no_email)
    
    if report_path:
        print(f"\nâœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    else:
        print("\nâš ï¸ æœªç”ŸæˆæŠ¥å‘Š")


if __name__ == "__main__":
    main()
